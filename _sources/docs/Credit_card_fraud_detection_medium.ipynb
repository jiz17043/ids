{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad77f919",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2494d0a",
   "metadata": {},
   "source": [
    "Our goal is to identify fraudulent transactions using classification models to classify and distinguish them.\n",
    "\n",
    "Steps involved:\n",
    "- Importing the required packages\n",
    "- Importing the data\n",
    "- Exploratory Data Analysis\n",
    "- Data Split\n",
    "- Building various classification models\n",
    "- Evaluating the classification models using the evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c18ed4",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd578a0c",
   "metadata": {},
   "source": [
    "We will be using the following packages: Pandas to manipulate data, NumPy to manipulate arrays, scikit-learn for spliting the data into train and test, building and evaluating the classification models, and xgboost to use the xgboost classifier model algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f916a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0fc8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from termcolor import colored as cl # text customization\n",
    "import itertools # advanced tools\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3145f",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f74ba",
   "metadata": {},
   "source": [
    "The variables V1 to V28 in the data are the principal components obtained by PCA. Since the variable time does not help our modeling we are going to remove it from our data. The variable ‘Amount’ contains the total amount of money being transacted and the variable ‘Class’ contains the information whether the transaction is a fraudulent one or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012468e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/creditcard.csv')\n",
    "data.drop('Time', axis = 1, inplace = True)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978b3de",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53c9fb6",
   "metadata": {},
   "source": [
    "Next we look at the number of fraudulent and non-fraudulent cases that are present in our dataset and also compute the percentage of fraudulent cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = len(data)\n",
    "nonfraud_count = len(data[data.Class == 0])\n",
    "fraud_count = len(data[data.Class == 1])\n",
    "fraud_percentage = round(fraud_count/nonfraud_count*100, 2)\n",
    "\n",
    "print(cl('CASE COUNT', attrs = ['bold']))\n",
    "print(cl('--------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Total number of cases are {}'.format(cases), attrs = ['bold']))\n",
    "print(cl('Number of Non-fraud cases are {}'.format(nonfraud_count), attrs = ['bold']))\n",
    "print(cl('Number of fraud cases are {}'.format(fraud_count), attrs = ['bold']))\n",
    "print(cl('Percentage of fraud cases is {}'.format(fraud_percentage), attrs = ['bold']))\n",
    "print(cl('--------------------------------------------', attrs = ['bold']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "nonfraud_cases = data[data.Class == 0]\n",
    "fraud_cases = data[data.Class == 1]\n",
    "\n",
    "print(cl('CASE AMOUNT STATISTICS', attrs = ['bold']))\n",
    "print(cl('--------------------------------------------', attrs = ['bold']))\n",
    "print(cl('NON-FRAUD CASE AMOUNT STATS', attrs = ['bold']))\n",
    "print(nonfraud_cases.Amount.describe())\n",
    "print(cl('--------------------------------------------', attrs = ['bold']))\n",
    "print(cl('FRAUD CASE AMOUNT STATS', attrs = ['bold']))\n",
    "print(fraud_cases.Amount.describe())\n",
    "print(cl('--------------------------------------------', attrs = ['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10251600",
   "metadata": {},
   "source": [
    "We can see that the values in the variable ‘Amount’ has a high variability in comparison to the other variables. In order to tackle this issue we are going to standardize this variable using the ‘StandardScaler’ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "amount = data['Amount'].values\n",
    "\n",
    "data['Amount'] = sc.fit_transform(amount.reshape(-1, 1))\n",
    "\n",
    "print(cl(data['Amount'].head(10), attrs = ['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66ab97",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Class', axis = 1).values\n",
    "y = data['Class'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(cl('X_train samples : ', attrs = ['bold']), X_train[:1])\n",
    "print(cl('X_test samples : ', attrs = ['bold']), X_test[0:1])\n",
    "print(cl('y_train samples : ', attrs = ['bold']), y_train[0:20])\n",
    "print(cl('y_test samples : ', attrs = ['bold']), y_test[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9848c737",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb23e72",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83350d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')\n",
    "tree_model.fit(X_train, y_train)\n",
    "tree_yhat = tree_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ca214",
   "metadata": {},
   "source": [
    "The ‘DecisionTreeClassifier’ algorithm is used to build the model. And the ‘max_depth’ specification in the function refers to the number of splits in the tree and the ‘criterion’ specification determines when to stop splitting the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed5da7",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = n)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_yhat = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c9eb7",
   "metadata": {},
   "source": [
    "The ‘KNeighborsClassifier’ algorithm is used to build the model. The value of the ‘n_neighbors’ is chosen randomly but it can also be chosen optimistically by iterating through a range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5cc6df",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014747ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_yhat = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca318fb1",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d110cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "svm_yhat = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ad3cf",
   "metadata": {},
   "source": [
    "#### Random Forest Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f4de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth = 4)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_yhat = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44719a60",
   "metadata": {},
   "source": [
    "The ‘RandomForestClassifier’ algorithm is used to build the model and the ‘max_depth’ specification in the function refers to the number of splits in the tree. The main difference between the decision tree and the random forest is that, the decision tree uses the entire dataset to construct a single model whereas, the random forest uses randomly selected features to construct multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a110b72",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfcf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth = 4, use_label_encoder=False,eval_metric='error')\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_yhat = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568c9cf",
   "metadata": {},
   "source": [
    "The ‘XGBClassifier’ algorithm provided is used to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b4d35",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eef46a",
   "metadata": {},
   "source": [
    "#### Accuracy Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce213eb0",
   "metadata": {},
   "source": [
    "The accuracy score is calculated by dividing the number of correct predictions made by the model by the total number of predictions made by the model. It can generally be expressed as:\n",
    "\n",
    "Accuracy score = No.of correct predictions / Total no.of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cl('ACCURACY SCORE', attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test, tree_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the KNN model is {}'.format(accuracy_score(y_test, knn_yhat)), attrs = ['bold'], color = 'green'))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test, lr_yhat)), attrs = ['bold'], color = 'red'))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the SVM model is {}'.format(accuracy_score(y_test, svm_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the Random Forest Tree model is {}'.format(accuracy_score(y_test, rf_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('Accuracy score of the XGBoost model is {}'.format(accuracy_score(y_test, xgb_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3558d",
   "metadata": {},
   "source": [
    "According to the accuracy score evaluation metric, the KNN model seems to be the most accurate model and the Logistic regression model seems to be the least accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ebda5",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3021c8a",
   "metadata": {},
   "source": [
    "The F1 score or F-score can be defined as the harmonic mean of the model’s precision and recall. It is calculated by dividing the product of the model’s precision and recall by the value obtained on adding the model’s precision and recall and finally multiplying the result with 2 (We are giving equal weights to precision and recall). It can be expressed as:\n",
    "\n",
    "F1 score = 2( (precision * recall) / (precision + recall) )\n",
    "\n",
    "Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "Recall = True Positive / (True Positive + False Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fbc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cl('F1 SCORE', attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('F1 score of the Decision Tree model is {}'.format(f1_score(y_test, tree_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('F1 score of the KNN model is {}'.format(f1_score(y_test, knn_yhat)), attrs = ['bold'], color = 'green'))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('F1 score of the Logistic Regression model is {}'.format(f1_score(y_test, lr_yhat)), attrs = ['bold'], color = 'red'))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('F1 score of the SVM model is {}'.format(f1_score(y_test, svm_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('F1 score of the Random Forest Tree model is {}'.format(f1_score(y_test, rf_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))\n",
    "print(cl('F1 score of the XGBoost model is {}'.format(f1_score(y_test, xgb_yhat)), attrs = ['bold']))\n",
    "print(cl('------------------------------------------------------------------------', attrs = ['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d13a0a1",
   "metadata": {},
   "source": [
    "On basis of the F1 score evaluation metric, the KNN model is the best performing model and the Logistic regression model seems to be the least accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c33a4a0",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8555b",
   "metadata": {},
   "source": [
    "Typically, a confusion matrix is a visualization of a classification model that shows how well the model has predicted the outcomes when compared to the original ones. Usually, the predicted outcomes are stored in a variable that is then converted into a correlation table. Using the correlation table, the confusion matrix is plotted in the form of a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the plot function\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title, normalize = False, cmap = plt.cm.Blues):\n",
    "    title = 'Confusion Matrix of {}'.format(title)\n",
    "    if normalize:\n",
    "        cm = cm.astype(float) / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment = 'center',\n",
    "                 color = 'white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix for the models\n",
    "\n",
    "tree_matrix = confusion_matrix(y_test, tree_yhat, labels = [0, 1]) # Decision Tree\n",
    "knn_matrix = confusion_matrix(y_test, knn_yhat, labels = [0, 1]) # K-Nearest Neighbors\n",
    "lr_matrix = confusion_matrix(y_test, lr_yhat, labels = [0, 1]) # Logistic Regression\n",
    "svm_matrix = confusion_matrix(y_test, svm_yhat, labels = [0, 1]) # Support Vector Machine\n",
    "rf_matrix = confusion_matrix(y_test, rf_yhat, labels = [0, 1]) # Random Forest Tree\n",
    "xgb_matrix = confusion_matrix(y_test, xgb_yhat, labels = [0, 1]) # XGBoost\n",
    "\n",
    "# Plot the confusion matrix\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485147f9",
   "metadata": {},
   "source": [
    "#### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7232150",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_cm_plot = plot_confusion_matrix(tree_matrix, \n",
    "                                classes = ['Non-Default(0)','Default(1)'], \n",
    "                                normalize = False, title = 'Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490d8bb",
   "metadata": {},
   "source": [
    "The first row represents transactions whose actual fraud value in the test dataset is 0. As we can see, the total number of non-fraud transactions is 56861. And out of these 56861 non-fraud transactions, the classifier correctly predicted 56849 of them as 0 and 12 of them as 1.\n",
    "\n",
    "Let’s look at the second row. It looks like there were 101 fraudulent transactions. The classifier correctly predicted 77 of them as 1, and 24 of them wrongly as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c45fb1",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bed321",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cm_plot = plot_confusion_matrix(knn_matrix, \n",
    "                                classes = ['Non-Default(0)','Default(1)'], \n",
    "                                normalize = False, title = 'KNN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb266c",
   "metadata": {},
   "source": [
    "Out of these 56861 non-fraud transactions, the classifier correctly predicted 56854 of them as 0 and 7 of them as 1. \n",
    "\n",
    "Out of 101 fraudulent cases the classifier correctly predicted 81 of them as 1, and 20 of them wrongly as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d80157e",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfddf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cm_plot = plot_confusion_matrix(lr_matrix, \n",
    "                                classes = ['Non-Default(0)','Default(1)'], \n",
    "                                normalize = False, title = 'Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e5294",
   "metadata": {},
   "source": [
    "Out of these 56861 non-fraud transactions, the classifier correctly predicted 56852 of them as 0 and 9 of them as 1. \n",
    "\n",
    "Out of 101 fraudulent cases the classifier correctly predicted 64 of them as 1, and 37 of them wrongly as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2f729",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c9859",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_cm_plot = plot_confusion_matrix(svm_matrix, \n",
    "                                classes = ['Non-Default(0)','Default(1)'], \n",
    "                                normalize = False, title = 'SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6201d77",
   "metadata": {},
   "source": [
    "Out of these 56861 non-fraud transactions, the classifier correctly predicted 56855 of them as 0 and 6 of them as 1. \n",
    "\n",
    "Out of 101 fraudulent cases the classifier correctly predicted 68 of them as 1, and 33 of them wrongly as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b60c418",
   "metadata": {},
   "source": [
    "#### Random forest tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cm_plot = plot_confusion_matrix(rf_matrix, \n",
    "                                classes = ['Non-Default(0)','Default(1)'], \n",
    "                                normalize = False, title = 'Random Forest Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75230bc4",
   "metadata": {},
   "source": [
    "Out of these 56861 non-fraud transactions, the classifier correctly predicted 56854 of them as 0 and 7 of them as 1. \n",
    "\n",
    "Out of 101 fraudulent cases the classifier correctly predicted 69 of them as 1, and 32 of them wrongly as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2a169",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cm_plot = plot_confusion_matrix(xgb_matrix, \n",
    "                                classes = ['Non-Default(0)','Default(1)'], \n",
    "                                normalize = False, title = 'XGBoost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe64cf",
   "metadata": {},
   "source": [
    "Out of these 56861 non-fraud transactions, the classifier correctly predicted 56854 of them as 0 and 7 of them as 1. \n",
    "\n",
    "Out of 101 fraudulent cases the classifier correctly predicted 79 of them as 1, and 22 of them wrongly as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9b549",
   "metadata": {},
   "source": [
    "Comparing the confusion matrix of all the models, it can be seen that the K-Nearest Neighbors model has performed really well in classifying the fraud transactions from the non-fraud transactions followed. So we can conclude that the most appropriate model which can be used for our case is the K-Nearest Neighbors model and the least appropraite model is the Logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835dad2e",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- Credit Card Fraud Detection With Machine Learning in Python by Nikhil Adityan\n",
    "  https://medium.com/codex/credit-card-fraud-detection-with-machine-learning-in-python-ac7281991d87"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   13,
   17,
   29,
   33,
   37,
   42,
   61,
   65,
   69,
   74,
   78,
   82,
   97,
   111,
   115,
   122,
   126,
   136,
   140,
   144,
   148,
   152,
   156,
   162,
   166,
   170,
   174,
   178,
   182,
   186,
   190,
   194,
   198,
   202,
   206,
   210,
   214,
   220,
   235,
   239,
   243,
   253,
   268,
   272,
   276,
   280,
   318,
   322,
   327,
   333,
   337,
   342,
   348,
   352,
   357,
   363,
   367,
   372,
   378,
   382,
   387,
   393,
   397,
   402,
   408,
   412
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}