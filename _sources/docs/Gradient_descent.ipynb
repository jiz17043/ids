{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05cfe2d0",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "- Batch GD & Mini-batch GD\n",
    "- GD with momentum\n",
    "- Adam\n",
    "\n",
    "## Batch GD & Mini-batch GD\n",
    "The vanilla GD updates the parameters for entire dataset:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta\\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "SGD updates the parameters for each training examples.\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta\\nabla_\\theta J(\\theta;x^{(i)};y^{(i)})\n",
    "$$\n",
    "\n",
    "Mini-batch GD performs updates for every mini-batch.\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta\\nabla_\\theta J(\\theta;x^{(i:i+n)};y^{(i:i+n)})\n",
    "$$\n",
    "\n",
    "The Batch GD converges to a local minimum for non-convex problems and slow when the size of data is large.\\\n",
    "The SGD updates frequently with a high variance, causing the lost function fluctuate heavily.\\\n",
    "Mini-batch GD takes the advantage of both. Note that in training neural network models, term SGD is usually employed.\\\n",
    "**Challenges**:  Saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.\n",
    "\n",
    "## GD with momentum\n",
    "The momentum term increase for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions.\\\n",
    "\"Ball runing down a surface\"\\\n",
    "We we gain faster convergence.\n",
    "$$\n",
    "v_t = \\gamma v_{t-1}+ \\eta\\nabla_\\theta J(\\theta) \\\\\n",
    "\\theta = \\theta - v_t\n",
    "$$\n",
    "\n",
    "## Adam\n",
    "- Adagrad: adapts the learning rate\n",
    "- Adadelta: extension of Adagrad, more conservative \n",
    "- Adam: Adaptive Moment Estimation\n",
    "Previously, we updates for $\\theta$ using the same learning rate $\\eta$.\\\n",
    "The adaptive learning rate is changing with the second-order moment of gradient.\\\n",
    "\"Ball with friction\"\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1}+(1-\\beta_1)g_t\\\\\n",
    "v_t = \\beta_2{t-1}+(1-\\beta_2)g_t^2\n",
    "$$\n",
    "\n",
    "The $m_t,v_t$ serve as estimates for the gradient $g_t$. With some bias correction, the updates is performed as follow:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\frac{\\eta}{\\sqrt{\\hat{v}_t}+\\epsilon}\\hat{m}_t\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.13.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "source_map": [
   12
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}